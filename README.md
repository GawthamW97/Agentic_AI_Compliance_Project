# Agentic AI vs RAG for Automated HS Classification & Compliance

> **Production-ready AI research project demonstrating advanced RAG, Agentic AI, and LLM evaluation for regulated enterprise systems (ERP/CRM, trade & tax compliance).**

---

## ğŸš€ Why This Project Matters (Recruiter Summary)

This project demonstrates **end-to-end applied AI engineering**, not just model experimentation. It shows how Large Language Models can be **safely, reliably, and measurably deployed** in **complianceâ€‘critical enterprise environments**.

**What this proves:**
- You can design **AI systems, not just prompts**
- You understand **evaluation, metrics, and reproducibility**
- You can mitigate **hallucinations in real-world AI workflows**
- You can integrate AI into **ERP/CRM-style architectures**

---

## ğŸ§  Core Skills Demonstrated

**AI / ML**
- Retrieval-Augmented Generation (RAG)
- Agentic AI (multi-agent reasoning, hierarchical workflows)
- Hallucination detection & mitigation
- Embedding-based semantic search
- Prompt engineering & reasoning control

**Data & Evaluation**
- Experimental design & benchmarking
- Top-K accuracy, Precision, Recall, F1
- Latency & performance trade-off analysis
- Deterministic vs stochastic evaluation handling

**Engineering**
- Python modular architecture
- Vector databases (ChromaDB)
- YAML-based configuration management
- Structured logging (CSV / JSONL)
- Reproducible pipelines

**Enterprise & Compliance Context**
- HS / CN code classification
- EU VAT & TARIC alignment
- ERP/CRM integration patterns
- GDPR-aware system design

---

## ğŸ¯ Project Objective

To **compare and quantify** the effectiveness of three AI architectures for automated product classification:

1. **Baseline RAG** â€“ fast, retrieval-only
2. **RAG + LLM** â€“ retrieval with reasoning
3. **Agentic AI (Proposed)** â€“ self-verifying multi-agent system

The goal is to identify which approach delivers the **best balance of accuracy, reliability, and latency** for enterprise adoption.

---

## ğŸ—ï¸ High-Level Architecture

```text
Input Product Description
        â”‚
        â–¼
Embedding Model (OpenAI)
        â”‚
        â–¼
Vector DB (ChromaDB)
        â”‚
        â”œâ”€â”€ RAG (Retrieval Only)
        â”œâ”€â”€ RAG + LLM (Reasoning)
        â””â”€â”€ Agentic AI (Hierarchical Agents)
                â”‚
                â–¼
        Final HS/CN Code Prediction
```

---

## ğŸ“‚ Repository Structure

```text
Agentic_AI_Compliance_Project/
â”‚
â”œâ”€â”€ dataset/                # Synthetic & ground-truth datasets
â”œâ”€â”€ RAG/                    # Baseline & RAG+LLM pipelines
â”œâ”€â”€ Agentic/                # Multi-agent hierarchical pipeline
â”œâ”€â”€ config/                 # YAML-based configuration
â”œâ”€â”€ evaluation_logs/        # CSV & JSONL experiment outputs
â”œâ”€â”€ main.py                 # Unified experiment runner
â””â”€â”€ README.md
```

---

## ğŸ“Š Key Results (Headline)

| Architecture | Topâ€‘1 Accuracy | F1 Score | Reliability |
|-------------|---------------|---------|------------|
| RAG | Lowâ€“Medium | Low | Retrievalâ€‘dependent |
| RAG + LLM | Medium | Medium | Promptâ€‘sensitive |
| **Agentic AI** | **High** | **High** | **Most consistent** |

**Takeaway:** Agentic AI delivers **substantially better correctness and stability**, at the cost of higherâ€”but predictableâ€”latency.

---

## ğŸ§ª Evaluation Highlights

- Fixed test sets to ensure fair comparison
- Multiple Topâ€‘K retrieval depths (3, 7, 14, 20)
- Caseâ€‘based testing across description complexity
- Full experiment logs for auditability

This mirrors **industry-grade ML validation**, not academic toy experiments.

---


## ğŸ’¼ Real-World Use Cases

- Automated product classification in ERP systems
- VAT & TARIC validation pipelines
- AI-assisted customs & trade compliance
- Safer LLM deployment in regulated domains


---

## ğŸ‘¤ Author

**Gawtham Wayne**  
MSc Data Analytics â€“ National College of Ireland  
Focus: Applied AI, LLM Systems
